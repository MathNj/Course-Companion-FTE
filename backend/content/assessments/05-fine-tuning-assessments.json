{
  "chapter_id": "05-fine-tuning",
  "questions": [
    {
      "question_id": "05-ft-q1",
      "question_text": "When should you fine-tune an LLM versus using prompt engineering? Provide a decision framework with specific criteria and concrete examples where fine-tuning is justified.",
      "evaluation_criteria": [
        "Understanding of fine-tuning vs prompt engineering tradeoffs",
        "Specific criteria for when to fine-tune",
        "Concrete examples with justification",
        "Cost-benefit analysis framework"
      ],
      "example_excellent_answer": "Decision Framework: Fine-tune when (1) Format compliance needed (JSON, XML) - prompts work 70% of the time, fine-tuning achieves 99%, (2) Domain-specific writing style (medical notes, legal contracts), (3) Token efficiency - fine-tuned models use 50% fewer tokens for same task, (4) Latency critical - single call vs. chain-of-thought prompting. Example 1: Medical summarization. Prompt engineering requires 2000 tokens of instructions and examples, but still misses 15% of abbreviations. Fine-tuning on 10K medical notes reduces tokens to 500 and achieves 98% accuracy. ROI: $5K fine-tuning cost vs. $50K/year in token savings. Example 2: Code generation for internal framework. Prompts fail because model lacks framework knowledge. Fine-tuning on 5K framework examples enables correct API usage. DON'T fine-tune for: factual queries (use RAG), one-off tasks (prompts faster), or when training data is scarce (<1K examples). The decision hinges on (repeated use × token savings) > fine-tuning cost.",
      "example_poor_answer": "You should fine-tune for everything because it makes the model smarter. Prompt engineering is just talking to the AI and doesn't work as well as training it."
    },
    {
      "question_id": "05-ft-q2",
      "question_text": "You need to prepare a dataset for fine-tuning GPT-4o-mini. Describe your data collection, cleaning, and validation process. What are the most common pitfalls and how do you avoid them?",
      "evaluation_criteria": [
        "Data collection strategy (quality, diversity, volume)",
        "Data cleaning and preprocessing steps",
        "Validation and quality assurance methods",
        "Common pitfalls and mitigation strategies"
      ],
      "example_excellent_answer": "Data Collection: Target 5-10K high-quality examples minimum. Use production data if available (with privacy scrubbed), or synthetic data generated by GPT-4 and human-verified. Ensure diversity across use cases (don't train on only happy-path examples). For classification, balance classes. For generation, include edge cases and error handling. Cleaning: (1) Remove PII with regex + NER models, (2) Deduplicate (both exact and semantic), (3) Fix encoding issues (mojibake), (4) Normalize formats (dates, phone numbers), (5) Filter by quality (remove examples with low human ratings if applicable). Validation: Hold out 10% as test set. Run baseline model on test set before fine-tuning to establish improvement target. Have 2-3 domain experts review 100 random samples for accuracy. Common Pitfalls: (1) Data leakage - same examples in train/test, fix by hashing on user_id or content fingerprint, (2) Overfitting to train set - monitor test loss divergence, use early stopping, (3) Poisoned data from adversarial users - cap samples per user at 100, (4) Concept drift - data older than 6 months may be outdated, use time-based split. Validation: After fine-tuning, run on test set and target >15% improvement in accuracy or human preference scores.",
      "example_poor_answer": "Just collect as much data as possible, like 100 examples. Clean it by removing bad ones. Make sure it all looks good before uploading. Don't worry about duplicates."
    },
    {
      "question_id": "05-ft-q3",
      "question_text": "Compare LoRA (Low-Rank Adaptation) versus full fine-tuning. When would you use each approach, and what are the tradeoffs in terms of cost, performance, and deployment complexity?",
      "evaluation_criteria": [
        "Understanding of LoRA vs full fine-tuning mechanics",
        "Cost comparison (compute, storage, time)",
        "Performance differences in accuracy",
        "Deployment and production considerations"
      ],
      "example_excellent_answer": "LoRA adds trainable rank decomposition matrices to model weights, training only 0.1-1% of parameters. Full fine-tuning updates all weights. LoRA costs $50-200 to train on 10K examples (A100 8GB, 2 hours), while full fine-tuning costs $500-2000 (8×A100 80GB, 8 hours). Storage: LoRA adapters are 10-100MB per model vs. full checkpoints at 10-30GB. Performance: Full fine-tuning achieves 2-5% better accuracy on complex tasks but LoRA matches within 1-2% for most use cases. Deployment: LoRA requires loading base model + adapter (still needs full GPU memory), but you can swap adapters for different tasks without reloading the base model. Use LoRA when: (1) Rapid experimentation (test 10 task adapters in parallel), (2) Multi-tenant SaaS (different adapter per customer), (3) Limited compute budget, (4) Need to switch between tasks quickly. Use full fine-tuning when: (1) Maximum accuracy required (medical diagnosis, legal analysis), (2) Model will serve a single task at scale, (3) Can amortize training cost over high usage. Production tip: Most production systems start with LoRA for experimentation, then graduate to full fine-tuning only if accuracy gap justifies 10× cost.",
      "example_poor_answer": "LoRA is better because it's faster and cheaper. Full fine-tuning is too expensive and nobody uses it anymore. Always use LoRA for everything."
    }
  ]
}
