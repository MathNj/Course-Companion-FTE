# Data Model: Phase 2 - Hybrid Intelligence

**Feature Branch**: `003-phase-2-hybrid-intelligence`
**Created**: 2026-01-24
**Related Artifacts**: [spec.md](./spec.md) | [plan.md](./plan.md) | [contracts/README.md](./contracts/README.md)

---

## Overview

Phase 2 extends the Phase 1 database schema with 5 new tables to support LLM-powered premium features:
- **adaptive_paths**: Stores personalized learning recommendations
- **assessment_submissions**: Stores student open-ended answers
- **assessment_feedback**: Stores LLM-generated grading and feedback
- **llm_usage_logs**: Tracks every LLM API call with cost data
- **premium_usage_quotas**: Enforces monthly rate limits per premium student

**Key Principles**:
- All tables reference Phase 1's `students` table via foreign keys
- JSONB columns store structured LLM outputs for flexible querying
- Timestamps track creation and expiration for caching/auditing
- Cost tracking is per-request for granular analysis
- Indexes optimize common query patterns (student lookups, date ranges)

---

## Phase 2 Database Schema

### Table: `adaptive_paths`

Stores personalized learning recommendations generated by Claude Sonnet 4.5 based on student performance patterns.

**SQL Schema**:
```sql
CREATE TABLE adaptive_paths (
    -- Primary Key
    path_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Foreign Keys
    student_id UUID NOT NULL REFERENCES students(student_id) ON DELETE CASCADE,

    -- Metadata
    generated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP NOT NULL DEFAULT (CURRENT_TIMESTAMP + INTERVAL '24 hours'),

    -- Recommendations (JSONB)
    recommendations_json JSONB NOT NULL,
    -- Structure: [
    --   {
    --     "chapter_id": "04-rag",
    --     "section_id": "embeddings-review",
    --     "priority": 1,
    --     "reason": "Your quiz scores show weak understanding of vector embeddings, which are foundational for RAG",
    --     "estimated_impact": "high",
    --     "estimated_time_minutes": 30
    --   },
    --   ...
    -- ]

    reasoning TEXT NOT NULL, -- Why these recommendations were generated

    -- Cost Tracking
    tokens_input INTEGER NOT NULL,
    tokens_output INTEGER NOT NULL,
    cost_usd DECIMAL(10, 6) NOT NULL,

    -- Status
    status VARCHAR(20) NOT NULL DEFAULT 'active' CHECK (status IN ('active', 'expired', 'superseded')),
    followed_at TIMESTAMP, -- When student started following recommendations
    completed_at TIMESTAMP, -- When student finished all recommendations

    -- Indexes
    CONSTRAINT valid_expiration CHECK (expires_at > generated_at)
);

-- Indexes for Performance
CREATE INDEX idx_adaptive_paths_student_id ON adaptive_paths(student_id);
CREATE INDEX idx_adaptive_paths_generated_at ON adaptive_paths(generated_at DESC);
CREATE INDEX idx_adaptive_paths_status ON adaptive_paths(status) WHERE status = 'active';
CREATE INDEX idx_adaptive_paths_expires_at ON adaptive_paths(expires_at) WHERE status = 'active';

-- Composite index for cache lookups
CREATE INDEX idx_adaptive_paths_student_active ON adaptive_paths(student_id, status, expires_at)
    WHERE status = 'active';
```

**Entity-Relationship**:
```
students (Phase 1)
    1 ──< adaptive_paths
    (one student has many adaptive paths over time)
```

**Caching Strategy**:
- TTL: 24 hours (86400 seconds)
- Redis key: `adaptive_path:{student_id}` → JSON serialized recommendations
- Invalidation: On new quiz completion (score change >20%) or explicit `force_refresh=true` flag
- Database remains source of truth; Redis is read-through cache

---

### Table: `assessment_submissions`

Stores student-written answers to open-ended assessment questions.

**SQL Schema**:
```sql
CREATE TABLE assessment_submissions (
    -- Primary Key
    submission_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Foreign Keys
    student_id UUID NOT NULL REFERENCES students(student_id) ON DELETE CASCADE,
    question_id VARCHAR(50) NOT NULL, -- e.g., "04-rag-q1" (references content in R2)

    -- Submission Data
    answer_text TEXT NOT NULL CHECK (LENGTH(answer_text) BETWEEN 50 AND 5000),
    submitted_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,

    -- Grading Status
    grading_status VARCHAR(20) NOT NULL DEFAULT 'pending'
        CHECK (grading_status IN ('pending', 'processing', 'completed', 'failed')),
    grading_started_at TIMESTAMP,
    grading_completed_at TIMESTAMP,

    -- Retry Tracking
    attempt_number INTEGER NOT NULL DEFAULT 1, -- 1st, 2nd, 3rd attempt
    previous_submission_id UUID REFERENCES assessment_submissions(submission_id),

    -- Error Handling
    error_message TEXT, -- Populated if grading_status = 'failed'

    -- Indexes
    CONSTRAINT valid_grading_timeline CHECK (
        grading_completed_at IS NULL OR grading_started_at IS NULL OR grading_completed_at >= grading_started_at
    ),
    CONSTRAINT valid_attempt_number CHECK (attempt_number BETWEEN 1 AND 3)
);

-- Indexes for Performance
CREATE INDEX idx_submissions_student_id ON assessment_submissions(student_id);
CREATE INDEX idx_submissions_question_id ON assessment_submissions(question_id);
CREATE INDEX idx_submissions_status ON assessment_submissions(grading_status);
CREATE INDEX idx_submissions_submitted_at ON assessment_submissions(submitted_at DESC);

-- Composite index for student submission history
CREATE INDEX idx_submissions_student_question ON assessment_submissions(student_id, question_id, submitted_at DESC);
```

**Entity-Relationship**:
```
students (Phase 1)
    1 ──< assessment_submissions
    (one student has many submissions)

assessment_submissions (self-referencing for retries)
    1 ──< assessment_submissions.previous_submission_id
    (one submission may have subsequent retry submissions)
```

---

### Table: `assessment_feedback`

Stores LLM-generated grading feedback for assessment submissions.

**SQL Schema**:
```sql
CREATE TABLE assessment_feedback (
    -- Primary Key
    feedback_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Foreign Keys
    submission_id UUID NOT NULL REFERENCES assessment_submissions(submission_id) ON DELETE CASCADE,

    -- Grading Results
    quality_score DECIMAL(3, 1) NOT NULL CHECK (quality_score BETWEEN 0 AND 10),

    -- Structured Feedback (JSONB)
    strengths_json JSONB NOT NULL,
    -- Structure: [
    --   "Strong explanation of cost tradeoffs between RAG and fine-tuning",
    --   "Good use of concrete examples with data privacy considerations",
    --   "Demonstrated understanding of when to combine both approaches"
    -- ]

    improvements_json JSONB NOT NULL,
    -- Structure: [
    --   "Missing discussion of knowledge freshness requirements",
    --   "Could expand on computational cost differences",
    --   "Review Chapter 4, Section 3 for latency considerations"
    -- ]

    detailed_feedback TEXT NOT NULL, -- Full paragraph-form feedback

    -- Metadata
    generated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,

    -- Cost Tracking
    tokens_input INTEGER NOT NULL,
    tokens_output INTEGER NOT NULL,
    cost_usd DECIMAL(10, 6) NOT NULL,

    -- Quality Assurance
    human_reviewed BOOLEAN DEFAULT FALSE,
    human_reviewer_id UUID REFERENCES students(student_id), -- Admin/instructor
    human_review_notes TEXT,

    -- Constraints
    CONSTRAINT valid_strengths CHECK (jsonb_array_length(strengths_json) BETWEEN 1 AND 5),
    CONSTRAINT valid_improvements CHECK (jsonb_array_length(improvements_json) BETWEEN 1 AND 5)
);

-- Indexes for Performance
CREATE INDEX idx_feedback_submission_id ON assessment_feedback(submission_id);
CREATE INDEX idx_feedback_generated_at ON assessment_feedback(generated_at DESC);
CREATE INDEX idx_feedback_score ON assessment_feedback(quality_score);

-- Index for quality monitoring
CREATE INDEX idx_feedback_unreviewed ON assessment_feedback(human_reviewed, generated_at)
    WHERE human_reviewed = FALSE;
```

**Entity-Relationship**:
```
assessment_submissions
    1 ──< assessment_feedback
    (one submission has one feedback record)

students (Phase 1 - admins)
    1 ──< assessment_feedback.human_reviewer_id
    (one admin may review many feedbacks)
```

**JSONB Query Examples**:
```sql
-- Find all submissions with specific strength
SELECT * FROM assessment_feedback
WHERE strengths_json @> '["Strong explanation of cost tradeoffs"]';

-- Count common improvement suggestions
SELECT
    jsonb_array_elements_text(improvements_json) AS suggestion,
    COUNT(*) AS frequency
FROM assessment_feedback
GROUP BY suggestion
ORDER BY frequency DESC
LIMIT 10;
```

---

### Table: `llm_usage_logs`

Tracks every LLM API call with detailed cost and performance metrics for audit and optimization.

**SQL Schema**:
```sql
CREATE TABLE llm_usage_logs (
    -- Primary Key
    log_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Foreign Keys
    student_id UUID NOT NULL REFERENCES students(student_id) ON DELETE CASCADE,

    -- Feature Tracking
    feature VARCHAR(50) NOT NULL CHECK (feature IN ('adaptive-path', 'assessment')),
    reference_id UUID, -- FK to adaptive_paths.path_id or assessment_feedback.feedback_id

    -- Request Details
    request_timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    model_version VARCHAR(100) NOT NULL DEFAULT 'claude-sonnet-4-5-20250929',

    -- Token Usage
    tokens_input INTEGER NOT NULL CHECK (tokens_input > 0),
    tokens_output INTEGER NOT NULL CHECK (tokens_output > 0),
    tokens_total INTEGER GENERATED ALWAYS AS (tokens_input + tokens_output) STORED,

    -- Cost Calculation
    cost_usd DECIMAL(10, 6) NOT NULL CHECK (cost_usd > 0),
    -- Calculated as: (tokens_input * $3/1M) + (tokens_output * $15/1M)

    -- Performance
    latency_ms INTEGER NOT NULL CHECK (latency_ms >= 0), -- API call duration

    -- Status
    success BOOLEAN NOT NULL DEFAULT TRUE,
    error_code VARCHAR(50), -- e.g., "rate_limit_exceeded", "timeout"
    error_message TEXT,

    -- Data Retention
    deleted_at TIMESTAMP, -- Soft delete after 90 days

    -- Indexes
    CONSTRAINT valid_error_state CHECK (
        (success = TRUE AND error_code IS NULL) OR
        (success = FALSE AND error_code IS NOT NULL)
    )
);

-- Indexes for Performance
CREATE INDEX idx_llm_logs_student_id ON llm_usage_logs(student_id);
CREATE INDEX idx_llm_logs_feature ON llm_usage_logs(feature);
CREATE INDEX idx_llm_logs_timestamp ON llm_usage_logs(request_timestamp DESC);
CREATE INDEX idx_llm_logs_success ON llm_usage_logs(success);

-- Composite indexes for analytics queries
CREATE INDEX idx_llm_logs_student_month ON llm_usage_logs(
    student_id,
    DATE_TRUNC('month', request_timestamp)
) WHERE deleted_at IS NULL;

CREATE INDEX idx_llm_logs_feature_month ON llm_usage_logs(
    feature,
    DATE_TRUNC('month', request_timestamp)
) WHERE deleted_at IS NULL AND success = TRUE;

-- Partial index for active logs (not soft-deleted)
CREATE INDEX idx_llm_logs_active ON llm_usage_logs(request_timestamp DESC)
    WHERE deleted_at IS NULL;
```

**Entity-Relationship**:
```
students (Phase 1)
    1 ──< llm_usage_logs
    (one student has many LLM usage logs)

adaptive_paths
    1 ──< llm_usage_logs.reference_id (when feature='adaptive-path')

assessment_feedback
    1 ──< llm_usage_logs.reference_id (when feature='assessment')
```

**Cost Aggregation Queries**:
```sql
-- Monthly cost per student
SELECT
    student_id,
    DATE_TRUNC('month', request_timestamp) AS month,
    COUNT(*) AS total_requests,
    SUM(CASE WHEN feature = 'adaptive-path' THEN 1 ELSE 0 END) AS adaptive_paths_used,
    SUM(CASE WHEN feature = 'assessment' THEN 1 ELSE 0 END) AS assessments_used,
    SUM(cost_usd) AS total_cost_usd,
    AVG(latency_ms) AS avg_latency_ms
FROM llm_usage_logs
WHERE deleted_at IS NULL AND success = TRUE
GROUP BY student_id, DATE_TRUNC('month', request_timestamp)
ORDER BY total_cost_usd DESC;

-- Alert: Students exceeding $0.50/month threshold
SELECT
    student_id,
    SUM(cost_usd) AS monthly_cost
FROM llm_usage_logs
WHERE DATE_TRUNC('month', request_timestamp) = DATE_TRUNC('month', CURRENT_TIMESTAMP)
    AND deleted_at IS NULL
    AND success = TRUE
GROUP BY student_id
HAVING SUM(cost_usd) > 0.50;
```

---

### Table: `premium_usage_quotas`

Enforces monthly rate limits for premium features (10 adaptive paths, 20 assessments per premium user per month).

**SQL Schema**:
```sql
CREATE TABLE premium_usage_quotas (
    -- Primary Key
    quota_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Foreign Keys
    student_id UUID NOT NULL REFERENCES students(student_id) ON DELETE CASCADE,

    -- Time Period
    month DATE NOT NULL, -- First day of month (e.g., '2026-01-01')
    reset_date DATE NOT NULL, -- First day of next month

    -- Adaptive Path Quotas
    adaptive_paths_used INTEGER NOT NULL DEFAULT 0 CHECK (adaptive_paths_used >= 0),
    adaptive_paths_limit INTEGER NOT NULL DEFAULT 10 CHECK (adaptive_paths_limit > 0),

    -- Assessment Quotas
    assessments_used INTEGER NOT NULL DEFAULT 0 CHECK (assessments_used >= 0),
    assessments_limit INTEGER NOT NULL DEFAULT 20 CHECK (assessments_limit > 0),

    -- Metadata
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,

    -- Unique Constraint
    CONSTRAINT unique_student_month UNIQUE (student_id, month),

    -- Validation
    CONSTRAINT valid_month CHECK (month = DATE_TRUNC('month', month)::DATE),
    CONSTRAINT valid_reset_date CHECK (reset_date = (month + INTERVAL '1 month')::DATE)
);

-- Indexes for Performance
CREATE INDEX idx_quotas_student_id ON premium_usage_quotas(student_id);
CREATE INDEX idx_quotas_month ON premium_usage_quotas(month);
CREATE INDEX idx_quotas_reset_date ON premium_usage_quotas(reset_date);

-- Composite index for quota checks
CREATE INDEX idx_quotas_student_month ON premium_usage_quotas(student_id, month);

-- Trigger to auto-update updated_at
CREATE TRIGGER update_premium_quotas_updated_at
BEFORE UPDATE ON premium_usage_quotas
FOR EACH ROW
EXECUTE FUNCTION update_modified_column();
```

**Entity-Relationship**:
```
students (Phase 1)
    1 ──< premium_usage_quotas
    (one student has one quota record per month)
```

**Quota Check Query**:
```sql
-- Check if student can request adaptive path
SELECT
    adaptive_paths_used < adaptive_paths_limit AS can_request_path,
    adaptive_paths_limit - adaptive_paths_used AS remaining_paths
FROM premium_usage_quotas
WHERE student_id = :student_id
    AND month = DATE_TRUNC('month', CURRENT_DATE)::DATE;

-- Increment usage after successful request
UPDATE premium_usage_quotas
SET
    adaptive_paths_used = adaptive_paths_used + 1,
    updated_at = CURRENT_TIMESTAMP
WHERE student_id = :student_id
    AND month = DATE_TRUNC('month', CURRENT_DATE)::DATE;
```

**Redis Caching for Quotas**:
- Redis key: `quota:{student_id}:{YYYY-MM}:adaptive_paths` → integer count
- Redis key: `quota:{student_id}:{YYYY-MM}:assessments` → integer count
- TTL: Expires on first day of next month (auto-cleanup)
- Dual-write strategy: Increment Redis + PostgreSQL atomically

---

## Phase 1 Schema (Reference)

Phase 2 extends but does not modify Phase 1 tables. Key relationships:

**`students` Table (Phase 1 - Unchanged)**:
```sql
CREATE TABLE students (
    student_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    full_name VARCHAR(255),

    -- Subscription Management
    subscription_tier VARCHAR(20) NOT NULL DEFAULT 'free'
        CHECK (subscription_tier IN ('free', 'premium', 'pro')),
    subscription_expires_at TIMESTAMP,

    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);
```

**Foreign Key Relationships**:
```
students (Phase 1)
    ├── 1 ──< adaptive_paths (Phase 2)
    ├── 1 ──< assessment_submissions (Phase 2)
    ├── 1 ──< llm_usage_logs (Phase 2)
    └── 1 ──< premium_usage_quotas (Phase 2)
```

---

## Caching Strategy (Redis)

**Adaptive Paths (Read-Through Cache)**:
```
Key: adaptive_path:{student_id}
Value: JSON serialized recommendations_json + reasoning
TTL: 24 hours (86400 seconds)

Invalidation:
- On new quiz completion with score change >20%
- On explicit force_refresh=true flag
- Automatic expiration after 24h
```

**Usage Quotas (Write-Through Cache)**:
```
Key: quota:{student_id}:{YYYY-MM}:adaptive_paths
Value: Integer (current count)
TTL: Expires on 1st of next month

Key: quota:{student_id}:{YYYY-MM}:assessments
Value: Integer (current count)
TTL: Expires on 1st of next month

Operations:
- Check: GET quota key, compare to limit (10 paths, 20 assessments)
- Increment: INCR quota key + UPDATE PostgreSQL (atomic)
- Reset: Cron job on 1st of month creates new quota records
```

**Cache Consistency**:
- PostgreSQL is source of truth
- Redis failures fall back to database queries
- Dual-write ensures consistency (Redis + PostgreSQL updated together)

---

## Data Migration Strategy (Alembic)

### Migration 002: Adaptive Paths Table
**File**: `alembic/versions/002_phase2_adaptive_paths.py`

```python
"""Add adaptive_paths table for Phase 2

Revision ID: 002_phase2_adaptive_paths
Revises: 001_phase1_complete
Create Date: 2026-01-24
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = '002_phase2_adaptive_paths'
down_revision = '001_phase1_complete'

def upgrade():
    op.create_table(
        'adaptive_paths',
        sa.Column('path_id', postgresql.UUID(), nullable=False, server_default=sa.text('gen_random_uuid()')),
        sa.Column('student_id', postgresql.UUID(), nullable=False),
        sa.Column('generated_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.Column('expires_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text("CURRENT_TIMESTAMP + INTERVAL '24 hours'")),
        sa.Column('recommendations_json', postgresql.JSONB(), nullable=False),
        sa.Column('reasoning', sa.TEXT(), nullable=False),
        sa.Column('tokens_input', sa.INTEGER(), nullable=False),
        sa.Column('tokens_output', sa.INTEGER(), nullable=False),
        sa.Column('cost_usd', sa.DECIMAL(10, 6), nullable=False),
        sa.Column('status', sa.VARCHAR(20), nullable=False, server_default='active'),
        sa.Column('followed_at', sa.TIMESTAMP(), nullable=True),
        sa.Column('completed_at', sa.TIMESTAMP(), nullable=True),
        sa.PrimaryKeyConstraint('path_id'),
        sa.ForeignKeyConstraint(['student_id'], ['students.student_id'], ondelete='CASCADE'),
        sa.CheckConstraint("status IN ('active', 'expired', 'superseded')", name='check_status'),
        sa.CheckConstraint('expires_at > generated_at', name='valid_expiration')
    )

    op.create_index('idx_adaptive_paths_student_id', 'adaptive_paths', ['student_id'])
    op.create_index('idx_adaptive_paths_generated_at', 'adaptive_paths', ['generated_at'], postgresql_ops={'generated_at': 'DESC'})
    op.create_index('idx_adaptive_paths_status', 'adaptive_paths', ['status'], postgresql_where=sa.text("status = 'active'"))
    op.create_index('idx_adaptive_paths_student_active', 'adaptive_paths', ['student_id', 'status', 'expires_at'],
                   postgresql_where=sa.text("status = 'active'"))

def downgrade():
    op.drop_table('adaptive_paths')
```

### Migration 003: Assessment Tables
**File**: `alembic/versions/003_phase2_assessments.py`

```python
"""Add assessment_submissions and assessment_feedback tables for Phase 2

Revision ID: 003_phase2_assessments
Revises: 002_phase2_adaptive_paths
Create Date: 2026-01-24
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = '003_phase2_assessments'
down_revision = '002_phase2_adaptive_paths'

def upgrade():
    # assessment_submissions table
    op.create_table(
        'assessment_submissions',
        sa.Column('submission_id', postgresql.UUID(), nullable=False, server_default=sa.text('gen_random_uuid()')),
        sa.Column('student_id', postgresql.UUID(), nullable=False),
        sa.Column('question_id', sa.VARCHAR(50), nullable=False),
        sa.Column('answer_text', sa.TEXT(), nullable=False),
        sa.Column('submitted_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.Column('grading_status', sa.VARCHAR(20), nullable=False, server_default='pending'),
        sa.Column('grading_started_at', sa.TIMESTAMP(), nullable=True),
        sa.Column('grading_completed_at', sa.TIMESTAMP(), nullable=True),
        sa.Column('attempt_number', sa.INTEGER(), nullable=False, server_default='1'),
        sa.Column('previous_submission_id', postgresql.UUID(), nullable=True),
        sa.Column('error_message', sa.TEXT(), nullable=True),
        sa.PrimaryKeyConstraint('submission_id'),
        sa.ForeignKeyConstraint(['student_id'], ['students.student_id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['previous_submission_id'], ['assessment_submissions.submission_id']),
        sa.CheckConstraint("LENGTH(answer_text) BETWEEN 50 AND 5000", name='valid_answer_length'),
        sa.CheckConstraint("grading_status IN ('pending', 'processing', 'completed', 'failed')", name='check_grading_status'),
        sa.CheckConstraint('attempt_number BETWEEN 1 AND 3', name='valid_attempt_number'),
        sa.CheckConstraint('grading_completed_at IS NULL OR grading_started_at IS NULL OR grading_completed_at >= grading_started_at',
                          name='valid_grading_timeline')
    )

    op.create_index('idx_submissions_student_id', 'assessment_submissions', ['student_id'])
    op.create_index('idx_submissions_question_id', 'assessment_submissions', ['question_id'])
    op.create_index('idx_submissions_status', 'assessment_submissions', ['grading_status'])
    op.create_index('idx_submissions_student_question', 'assessment_submissions',
                   ['student_id', 'question_id', 'submitted_at'], postgresql_ops={'submitted_at': 'DESC'})

    # assessment_feedback table
    op.create_table(
        'assessment_feedback',
        sa.Column('feedback_id', postgresql.UUID(), nullable=False, server_default=sa.text('gen_random_uuid()')),
        sa.Column('submission_id', postgresql.UUID(), nullable=False),
        sa.Column('quality_score', sa.DECIMAL(3, 1), nullable=False),
        sa.Column('strengths_json', postgresql.JSONB(), nullable=False),
        sa.Column('improvements_json', postgresql.JSONB(), nullable=False),
        sa.Column('detailed_feedback', sa.TEXT(), nullable=False),
        sa.Column('generated_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.Column('tokens_input', sa.INTEGER(), nullable=False),
        sa.Column('tokens_output', sa.INTEGER(), nullable=False),
        sa.Column('cost_usd', sa.DECIMAL(10, 6), nullable=False),
        sa.Column('human_reviewed', sa.BOOLEAN(), nullable=False, server_default='false'),
        sa.Column('human_reviewer_id', postgresql.UUID(), nullable=True),
        sa.Column('human_review_notes', sa.TEXT(), nullable=True),
        sa.PrimaryKeyConstraint('feedback_id'),
        sa.ForeignKeyConstraint(['submission_id'], ['assessment_submissions.submission_id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['human_reviewer_id'], ['students.student_id']),
        sa.CheckConstraint('quality_score BETWEEN 0 AND 10', name='valid_score'),
        sa.CheckConstraint('jsonb_array_length(strengths_json) BETWEEN 1 AND 5', name='valid_strengths'),
        sa.CheckConstraint('jsonb_array_length(improvements_json) BETWEEN 1 AND 5', name='valid_improvements')
    )

    op.create_index('idx_feedback_submission_id', 'assessment_feedback', ['submission_id'])
    op.create_index('idx_feedback_generated_at', 'assessment_feedback', ['generated_at'], postgresql_ops={'generated_at': 'DESC'})
    op.create_index('idx_feedback_score', 'assessment_feedback', ['quality_score'])
    op.create_index('idx_feedback_unreviewed', 'assessment_feedback', ['human_reviewed', 'generated_at'],
                   postgresql_where=sa.text('human_reviewed = FALSE'))

def downgrade():
    op.drop_table('assessment_feedback')
    op.drop_table('assessment_submissions')
```

### Migration 004: Usage Tracking Tables
**File**: `alembic/versions/004_phase2_usage_tracking.py`

```python
"""Add llm_usage_logs and premium_usage_quotas tables for Phase 2

Revision ID: 004_phase2_usage_tracking
Revises: 003_phase2_assessments
Create Date: 2026-01-24
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = '004_phase2_usage_tracking'
down_revision = '003_phase2_assessments'

def upgrade():
    # llm_usage_logs table
    op.create_table(
        'llm_usage_logs',
        sa.Column('log_id', postgresql.UUID(), nullable=False, server_default=sa.text('gen_random_uuid()')),
        sa.Column('student_id', postgresql.UUID(), nullable=False),
        sa.Column('feature', sa.VARCHAR(50), nullable=False),
        sa.Column('reference_id', postgresql.UUID(), nullable=True),
        sa.Column('request_timestamp', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.Column('model_version', sa.VARCHAR(100), nullable=False, server_default='claude-sonnet-4-5-20250929'),
        sa.Column('tokens_input', sa.INTEGER(), nullable=False),
        sa.Column('tokens_output', sa.INTEGER(), nullable=False),
        sa.Column('tokens_total', sa.INTEGER(), sa.Computed('tokens_input + tokens_output'), nullable=False, stored=True),
        sa.Column('cost_usd', sa.DECIMAL(10, 6), nullable=False),
        sa.Column('latency_ms', sa.INTEGER(), nullable=False),
        sa.Column('success', sa.BOOLEAN(), nullable=False, server_default='true'),
        sa.Column('error_code', sa.VARCHAR(50), nullable=True),
        sa.Column('error_message', sa.TEXT(), nullable=True),
        sa.Column('deleted_at', sa.TIMESTAMP(), nullable=True),
        sa.PrimaryKeyConstraint('log_id'),
        sa.ForeignKeyConstraint(['student_id'], ['students.student_id'], ondelete='CASCADE'),
        sa.CheckConstraint("feature IN ('adaptive-path', 'assessment')", name='check_feature'),
        sa.CheckConstraint('tokens_input > 0', name='valid_input_tokens'),
        sa.CheckConstraint('tokens_output > 0', name='valid_output_tokens'),
        sa.CheckConstraint('cost_usd > 0', name='valid_cost'),
        sa.CheckConstraint('latency_ms >= 0', name='valid_latency'),
        sa.CheckConstraint('(success = TRUE AND error_code IS NULL) OR (success = FALSE AND error_code IS NOT NULL)',
                          name='valid_error_state')
    )

    op.create_index('idx_llm_logs_student_id', 'llm_usage_logs', ['student_id'])
    op.create_index('idx_llm_logs_feature', 'llm_usage_logs', ['feature'])
    op.create_index('idx_llm_logs_timestamp', 'llm_usage_logs', ['request_timestamp'], postgresql_ops={'request_timestamp': 'DESC'})
    op.create_index('idx_llm_logs_success', 'llm_usage_logs', ['success'])
    op.create_index('idx_llm_logs_active', 'llm_usage_logs', ['request_timestamp'],
                   postgresql_ops={'request_timestamp': 'DESC'}, postgresql_where=sa.text('deleted_at IS NULL'))

    # premium_usage_quotas table
    op.create_table(
        'premium_usage_quotas',
        sa.Column('quota_id', postgresql.UUID(), nullable=False, server_default=sa.text('gen_random_uuid()')),
        sa.Column('student_id', postgresql.UUID(), nullable=False),
        sa.Column('month', sa.DATE(), nullable=False),
        sa.Column('reset_date', sa.DATE(), nullable=False),
        sa.Column('adaptive_paths_used', sa.INTEGER(), nullable=False, server_default='0'),
        sa.Column('adaptive_paths_limit', sa.INTEGER(), nullable=False, server_default='10'),
        sa.Column('assessments_used', sa.INTEGER(), nullable=False, server_default='0'),
        sa.Column('assessments_limit', sa.INTEGER(), nullable=False, server_default='20'),
        sa.Column('created_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.Column('updated_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.PrimaryKeyConstraint('quota_id'),
        sa.ForeignKeyConstraint(['student_id'], ['students.student_id'], ondelete='CASCADE'),
        sa.UniqueConstraint('student_id', 'month', name='unique_student_month'),
        sa.CheckConstraint('adaptive_paths_used >= 0', name='valid_paths_used'),
        sa.CheckConstraint('adaptive_paths_limit > 0', name='valid_paths_limit'),
        sa.CheckConstraint('assessments_used >= 0', name='valid_assessments_used'),
        sa.CheckConstraint('assessments_limit > 0', name='valid_assessments_limit'),
        sa.CheckConstraint("month = DATE_TRUNC('month', month)::DATE", name='valid_month'),
        sa.CheckConstraint("reset_date = (month + INTERVAL '1 month')::DATE", name='valid_reset_date')
    )

    op.create_index('idx_quotas_student_id', 'premium_usage_quotas', ['student_id'])
    op.create_index('idx_quotas_month', 'premium_usage_quotas', ['month'])
    op.create_index('idx_quotas_student_month', 'premium_usage_quotas', ['student_id', 'month'])

def downgrade():
    op.drop_table('premium_usage_quotas')
    op.drop_table('llm_usage_logs')
```

---

## Entity-Relationship Diagram (Text)

```
┌─────────────────────┐
│ students (Phase 1)  │
│─────────────────────│
│ PK student_id       │
│    email            │
│    subscription_tier│
│    ...              │
└─────────────────────┘
         │
         ├──────────────────────────────────────┐
         │                                      │
         │ 1:N                                  │ 1:N
         ▼                                      ▼
┌─────────────────────────┐          ┌─────────────────────────┐
│ adaptive_paths (Phase 2)│          │ assessment_submissions  │
│─────────────────────────│          │─────────────────────────│
│ PK path_id              │          │ PK submission_id        │
│ FK student_id           │          │ FK student_id           │
│    recommendations_json │          │    question_id          │
│    reasoning            │          │    answer_text          │
│    tokens_input/output  │          │    grading_status       │
│    cost_usd             │          │    attempt_number       │
│    expires_at           │          │ FK previous_submission  │
└─────────────────────────┘          └─────────────────────────┘
                                               │
                                               │ 1:1
                                               ▼
                                     ┌─────────────────────────┐
                                     │ assessment_feedback     │
                                     │─────────────────────────│
                                     │ PK feedback_id          │
                                     │ FK submission_id        │
                                     │    quality_score        │
                                     │    strengths_json       │
                                     │    improvements_json    │
                                     │    detailed_feedback    │
                                     │    tokens_input/output  │
                                     │    cost_usd             │
                                     └─────────────────────────┘

┌─────────────────────────┐          ┌─────────────────────────┐
│ llm_usage_logs (Phase 2)│          │ premium_usage_quotas    │
│─────────────────────────│          │─────────────────────────│
│ PK log_id               │          │ PK quota_id             │
│ FK student_id           │          │ FK student_id           │
│    feature              │          │    month (YYYY-MM-01)   │
│    reference_id         │          │    adaptive_paths_used  │
│    tokens_input/output  │          │    adaptive_paths_limit │
│    cost_usd             │          │    assessments_used     │
│    latency_ms           │          │    assessments_limit    │
│    success              │          │    reset_date           │
└─────────────────────────┘          └─────────────────────────┘
         ▲                                     ▲
         │                                     │
         └──────────────┬──────────────────────┘
                        │
                        │ N:1
                  ┌─────────────────┐
                  │ students        │
                  │ (Phase 1)       │
                  └─────────────────┘
```

---

## Data Validation Rules

### Adaptive Paths
- `recommendations_json` must be valid JSON array with 1-5 recommendation objects
- Each recommendation must have: `chapter_id`, `section_id`, `priority` (1-5), `reason`, `estimated_impact` (high/medium/low), `estimated_time_minutes` (>0)
- `expires_at` must be >24 hours after `generated_at`
- `status` transitions: `active` → `expired` (after 24h) or `superseded` (new path generated)

### Assessment Submissions
- `answer_text` must be 50-5000 characters (enforced by CHECK constraint)
- `grading_status` workflow: `pending` → `processing` → `completed` (or `failed`)
- `attempt_number` limited to 1-3 (students can retry up to 3 times)
- `previous_submission_id` must reference earlier submission for same question_id

### Assessment Feedback
- `quality_score` must be 0.0-10.0 with one decimal place
- `strengths_json` and `improvements_json` must each contain 1-5 string elements
- `detailed_feedback` must be non-empty paragraph (>50 characters)
- One feedback record per submission (1:1 relationship)

### LLM Usage Logs
- `tokens_input` and `tokens_output` must be >0
- `cost_usd` calculated as: `(tokens_input * 0.000003) + (tokens_output * 0.000015)`
- `success=false` requires `error_code` and `error_message`
- `deleted_at` set after 90 days for data retention compliance

### Premium Usage Quotas
- One record per student per month (enforced by UNIQUE constraint)
- `month` must be first day of month (2026-01-01, 2026-02-01, etc.)
- `reset_date` must be first day of next month
- Limits: `adaptive_paths_limit=10`, `assessments_limit=20` (configurable per tier)

---

## Performance Considerations

**Indexes for Common Queries**:
1. Student lookup: `idx_adaptive_paths_student_id`, `idx_submissions_student_id`, `idx_quotas_student_id`
2. Time-based queries: `idx_llm_logs_timestamp`, `idx_feedback_generated_at`
3. Cache lookups: `idx_adaptive_paths_student_active` (student + status + expires_at)
4. Quota checks: `idx_quotas_student_month` (composite index for monthly lookups)
5. Analytics: `idx_llm_logs_feature_month` (feature-based cost aggregation)

**Optimizations**:
- Partial indexes for active records only (`WHERE status = 'active'`, `WHERE deleted_at IS NULL`)
- Composite indexes for multi-column queries (student + month, student + question + date)
- JSONB GIN indexes for searching within `strengths_json`/`improvements_json` (optional, add if needed)
- Generated column for `tokens_total` (avoid repeated calculations)

**Scalability**:
- Expected volume: 1,000 premium students × 30 LLM calls/month = 30,000 logs/month
- Soft delete for `llm_usage_logs` after 90 days (keeps audit trail, reduces query size)
- Partition `llm_usage_logs` by month if volume exceeds 1M records (future optimization)

---

## Testing Data Models

**Unit Tests** (`tests/models/test_phase2_models.py`):
```python
def test_adaptive_path_creation():
    """Test creating adaptive path with valid data"""
    path = AdaptivePath(
        student_id=student.student_id,
        recommendations_json=[
            {
                "chapter_id": "04-rag",
                "section_id": "embeddings-review",
                "priority": 1,
                "reason": "Weak understanding of embeddings",
                "estimated_impact": "high",
                "estimated_time_minutes": 30
            }
        ],
        reasoning="Quiz scores show confusion about vector embeddings",
        tokens_input=1200,
        tokens_output=300,
        cost_usd=0.0091
    )
    db.add(path)
    db.commit()

    assert path.path_id is not None
    assert path.status == 'active'
    assert len(path.recommendations_json) == 1

def test_assessment_submission_constraints():
    """Test answer length constraints"""
    # Too short (<50 chars)
    with pytest.raises(IntegrityError):
        submission = AssessmentSubmission(
            student_id=student.student_id,
            question_id="04-rag-q1",
            answer_text="Too short"
        )
        db.add(submission)
        db.commit()

    # Valid length (50-5000 chars)
    submission = AssessmentSubmission(
        student_id=student.student_id,
        question_id="04-rag-q1",
        answer_text="A" * 100  # Valid
    )
    db.add(submission)
    db.commit()
    assert submission.submission_id is not None

def test_usage_quota_enforcement():
    """Test monthly quota limits"""
    quota = PremiumUsageQuota(
        student_id=student.student_id,
        month=date(2026, 1, 1),
        adaptive_paths_used=9,
        adaptive_paths_limit=10
    )
    db.add(quota)
    db.commit()

    # Can request 1 more
    assert quota.adaptive_paths_used < quota.adaptive_paths_limit

    # Increment to limit
    quota.adaptive_paths_used += 1
    db.commit()

    # Should be at limit now
    assert quota.adaptive_paths_used >= quota.adaptive_paths_limit
```

---

**Data Model Complete. See [contracts/README.md](./contracts/README.md) for API documentation.**
